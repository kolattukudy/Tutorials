{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorchtraindataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKKtcU4QoTVp8iluNLFznd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kolattukudy/Tutorials/blob/master/PyTorchtraindataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3vA5cC8sC9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "5ad3d4ed-4dc5-4f65-89c8-e20c6d6c4c5f"
      },
      "source": [
        "!pip install transformers\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t198BOzCrzb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.CRITICAL)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzAcrHVrsU8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "model = model.to(device)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptmTShxasWOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W52qsQBRsnpp",
        "colab_type": "text"
      },
      "source": [
        "PyTorch Dataset module for Short jokes dataset\n",
        "For fine-tuning the GPT2 model, I will use this Short Jokes dataset published on Kaggle. After each joke, I add \"<|endofext|>\" which is recognized by the GPT2 model as and end of text marker. The marker will allow me to concatenate many jokes in a single input sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vma9oZWsd1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "\n",
        "class JokesDataset(Dataset):\n",
        "    def __init__(self, jokes_dataset_path = 'sample_data/'):\n",
        "        super().__init__()\n",
        "\n",
        "        short_jokes_path = os.path.join(jokes_dataset_path, 'shortjokes.csv')\n",
        "\n",
        "        self.joke_list = []\n",
        "        self.end_of_text_token = \"<|endoftext|>\"\n",
        "        \n",
        "        with open(short_jokes_path) as csv_file:\n",
        "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "            \n",
        "            x = 0\n",
        "            for row in csv_reader:\n",
        "                joke_str = f\"JOKE:{row[1]}{self.end_of_text_token}\"\n",
        "                self.joke_list.append(joke_str)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.joke_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.joke_list[item]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSx5tYzLs6ZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = JokesDataset()\n",
        "joke_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37fk3Eoqtvl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 3e-5\n",
        "WARMUP_STEPS = 5000\n",
        "MAX_SEQ_LEN = 400\n",
        "from transformers import AdamW\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiNJAGOtuINn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC2Yjg6XuXMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQC4gPh0vTt_",
        "colab_type": "text"
      },
      "source": [
        "Train on Joke dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etsn7n6tuqd2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "561d91b2-8682-4838-da03-caf54c331e4d"
      },
      "source": [
        "model = model.to(device)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "#scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS, t_total = -1)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\n",
        "proc_seq_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "\n",
        "tmp_jokes_tens = None\n",
        "models_folder = \"trained_models\"\n",
        "if not os.path.exists(models_folder):\n",
        "    os.mkdir(models_folder)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
        "    \n",
        "    for idx,joke in enumerate(joke_loader):\n",
        "        \n",
        "        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
        "        joke_tens = torch.tensor(tokenizer.encode(joke[0])).unsqueeze(0).to(device)\n",
        "        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
        "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "            continue\n",
        "        \n",
        "        #The first joke sequence in the sequence\n",
        "        if not torch.is_tensor(tmp_jokes_tens):\n",
        "            tmp_jokes_tens = joke_tens\n",
        "            continue\n",
        "        else:\n",
        "            #The next joke does not fit in so we process the sequence and leave the last joke \n",
        "            #as the start for next sequence \n",
        "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "                work_jokes_tens = tmp_jokes_tens\n",
        "                tmp_jokes_tens = joke_tens\n",
        "            else:\n",
        "                #Add the joke to sequence, continue and try to add more\n",
        "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
        "                continue\n",
        "        ################## Sequence ready, process it trough the model ##################\n",
        "            \n",
        "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
        "        loss, logits = outputs[:2]                        \n",
        "        loss.backward()\n",
        "        sum_loss = sum_loss + loss.detach().data\n",
        "                       \n",
        "        proc_seq_count = proc_seq_count + 1\n",
        "        if proc_seq_count == BATCH_SIZE:\n",
        "            proc_seq_count = 0    \n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step() \n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        if batch_count == 100:\n",
        "            print(f\"sum loss {sum_loss}\")\n",
        "            batch_count = 0\n",
        "            sum_loss = 0.0\n",
        "    \n",
        "    # Store the model after each epoch to compare the performance of them\n",
        "    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_joker_{epoch}.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0 started==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec59Jc90u1HW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_EPOCH = 4\n",
        "\n",
        "models_folder = \"trained_models\"\n",
        "\n",
        "model_path = os.path.join(models_folder, f\"gpt2_medium_joker_{MODEL_EPOCH}.pt\")\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "jokes_output_file_path = f'generated_{MODEL_EPOCH}.jokes'\n",
        "\n",
        "model.eval()\n",
        "if os.path.exists(jokes_output_file_path):\n",
        "    os.remove(jokes_output_file_path)\n",
        "    \n",
        "joke_num = 0\n",
        "with torch.no_grad():\n",
        "   \n",
        "        for joke_idx in range(1000):\n",
        "        \n",
        "            joke_finished = False\n",
        "\n",
        "            cur_ids = torch.tensor(tokenizer.encode(\"JOKE:\")).unsqueeze(0).to(device)\n",
        "\n",
        "            for i in range(100):\n",
        "                outputs = model(cur_ids, labels=cur_ids)\n",
        "                loss, logits = outputs[:2]\n",
        "                softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
        "                if i < 3:\n",
        "                    n = 20\n",
        "                else:\n",
        "                    n = 3\n",
        "                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n",
        "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
        "\n",
        "                if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
        "                    joke_finished = True\n",
        "                    break\n",
        "\n",
        "            \n",
        "            if joke_finished:\n",
        "                \n",
        "                joke_num = joke_num + 1\n",
        "                \n",
        "                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "                output_text = tokenizer.decode(output_list)\n",
        "                print(output_text)\n",
        "                with open(jokes_output_file_path, 'a') as f:\n",
        "                    f.write(f\"{output_text} \\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}